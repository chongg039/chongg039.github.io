<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>summary on Co1driver 的个人博客</title>
    <link>https://chongg039.cn/categories/summary/</link>
    <description>Recent content in summary on Co1driver 的个人博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Copyright 2014-{year}</copyright>
    <lastBuildDate>Wed, 29 Jan 2020 19:51:25 +0800</lastBuildDate><atom:link href="https://chongg039.cn/categories/summary/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>2019 年终总结</title>
      <link>https://chongg039.cn/post/2019-summary/</link>
      <pubDate>Wed, 29 Jan 2020 19:51:25 +0800</pubDate>
      
      <guid>https://chongg039.cn/post/2019-summary/</guid>
      <description>看日期就知道，这篇总结其实是在2020年初，由于流感肆虐出不了门，在家里写了补上的。最近发生了许多大事，包括我最喜欢的NBA球星因意外离去，让人痛心。对于这些事情下面一概不谈，就简单回顾一下2019年我做的主要工作、收获，以及对未来的一些展望吧。
项目经历 首先是研究生期间的工作。从六月份的暑假开始研一结束，也标志着我们这一届正式开始做这个国家重点研发计划立项的项目。虽然已经是第二期了，但是由于中间断过档，之前的文档不完善，再加上老师要求全部换用新的技术栈，一开始也是很迷茫，不知道如何下手。幸得师兄和同门们的互相鼓励和帮助，让这个项目得以有了一个初步的雏形。我和一帮同志主要是负责项目的自然语言理解部分，也是最前端的部分，需要将文字叙述转化成我们能够使用的知识表述类型。鉴于有的部分涉及保密，这里只能说明一下大概的整个的摸索过程。
六到八月份，我主要是在用深度学习探索解决这方面问题的可能性，因为研一的时候自己摸索过、请教过一些问题，自己也对这些端到端的方法比较感兴趣。加上18-19年不是提出了Bert和Transformer嘛，没别的，就是强，就是好用。而我们这边的大类问题也无非就是一些传统的NER、RE还有指代消解什么的，就想在传统的标注网络和分类网络上搞搞创新，看看可不可行。不谦虚地说，那时候不屑于和他们做理论的人一样，拿一个写好的模型改一改，甚至调调参数，就写个论文发出去了。咱真是心气高，想把里面的原理彻底搞懂，就是生写，从最原始的来，手动求梯度，在纸上算好各个张量的维度，模仿别人搞数据预处理，再从头开始用TensorFlow一步一步编写各个网络模块，最后再去验证模型的效果。当然上面这些自夸都是玩笑话哈哈，回头想一想这也许就是我为什么发不出论文来了吧……不过说实在的，这段时间的经历，不断地去想去算、再去问师兄解答疑惑，最后再用代码真正将其一步一步实现出来，确实让我受益良多。直接导致的一个后果就是之后基本再出什么新的网络模型，都能比较快的去理解作者的本意，毕竟大部分是些换汤不换药的东西……
上面的话可能有些引战了，受到冒犯的朋友们还请不要当真哈哈。话说回来，为什么最后我们的项目中不以这些深度网络模型为主呢。随着项目的推进，我在其中发现的一个主要问题是，在我们这个限定领域内做知识表示和自然语言处理要求极度精确，就是像我们老师希望达到的是百分之百的准确那样。这和在开放领域当中不同，因为语言这个东西是因人而异的，再说从概率上说也不能保证事情发生的绝对正确性（我指的是在NLP领域中）。那么如何在限定领域内无限逼近这个要求呢，我想出来的方法就是让纠错的代价逼近在这个领域内能逼近的最小。这样说有些抽象了，其实就是如果系统出了错（不论是线下调整还是上线后），能够以最快的方式、最小的代价将问题修复，那么在一个有限的时间内，系统将是越来越精确的，也就是说是朝着百分之百精确的方向前进的，就可以了。
一个深度学习模型不可能达到这个效果，至少在我落笔的此刻我还没有见过这种类型的模型。假设我的分类网络用100万特定领域的语料训练出来了99.5%的准确度（我们就算假设这个是测试集准确率），现在遇到了一条错误的数据，如何让模型学习到这条数据？换言之如何让模型在原有的数据不受影响的情况下，将模型的精确度进一步提高。有人会说加数据，但加多少，如何加，又为什么能保证现有的正确的不会受到影响，至少现在的深度学习理论层面很难解释这个问题。
看到这里有人可能会觉得我说的这些问题，一看就是一个半吊子写出来的，根本不懂深度学习中的数学原理。的确，深度学习训练出来的模型本身就是一个概率模型，然后又妄图用非概率的方式去阐述它、应用它，是有些让人啼笑皆非。但我想说，这是我在具体业务中实践、感悟并总结出来的、仅适用于我们这个特殊项目的部分业务的一些经验。如果存在了某些出错但又必须要修正的情况，而方案本身修正它的代价又很大（就比如在这个项目中用神经网络做主要方案），那么能不能考虑一些其他的方案，将其作为新方案的补充，也未尝不可。
理清了这个思路，就是一步一步来，多看多尝试，最终敲定了一个比较合理的方案，也经过了项目组内部的高强度测试检验和完善。在这四个多月的过程中，我比较满意的是总结出了一套完整的在我们这个限定领域内的自然语言理解方案，重构了三次代码，剥离了许多子问题，并对这个领域有了更加深刻的认识。你说现在已经到了完美的程度了吗？当然没有，甚至说还远得很，但至少和我预想的一样，整体逻辑处于一种可控、易修正的状态，这已经令我很满意了。
从项目里面学到的另一个技能便是写文档。在项目进行到中期时，我越来越觉得维护一个内部的技术文档是多么的重要。不单单是为了将来的交接，也对自己回顾整理，乃至项目重构时有很大的帮助。代码注释在帮助理解项目整体思路上贡献甚微。因此我在内部服务器上搭建了一个gitbook作为项目文档，确实很有帮助。
这学期还做了一些内部零零散散的小项目，当然和这个主要的相比就显得有些微不足道了，也只是帮助了自己回顾了一下MySQL、TCP方面的一些知识。篇幅有限，不提也罢。
比赛 十月份初期拿了十天左右做了个天池的比赛，取得了一个还可以的成绩，之前有篇文章写过这部分内容，也不再啰嗦了。其实这不是第一个比赛，19年上半年做了个kaggle的，还有暑假做了个追一科技的，都不是很差当然也算不上好。虽然之后可能并不找算法为主的工作，但还是会做一下相关的比赛，一方面是增加经历，长长见识，另一方面还是要给枯燥的研究生生活增加点趣味性和挑战性。研究生研究生嘛，即便有项目不写论文，也不能老是重复着单调的工作。
之后的想法 首先是以后要做什么。很惭愧以前还说要做理论，要做算法，这还没有一年就发现自己不是这块料。当然如果以后的业务能是开发带一点算法，就更好了。有挑战性的工作就像玩游戏一样。
我在做这个项目的时候主要用的是python，这是出于两个因素。第一是许多深度学习的模型是用python写的，之前写的一些模块可以复用，也自然而然的用python编写现有的业务；第二是python确实简单易用，适合快速迭代开发。那时项目催得紧，用C++、Java不知道得开发到猴年马月，人手又少，所以也是赶鸭子上架。
现在项目迭代也几次了，也有时间回过头来审视语言层面的一些问题。这里我不是带语言优劣的节奏，就说一个很现实的问题，现在做开发，用python找不到工作啊！我本人在本科期间由JS、NodeJS、Golang一路做过来，就是因为本科期间C++没有学好，也成了我心中的一直以来的一个遗憾。既然上天给了我再一次选择的机会，我决定再向C++挑战一次。除了这些，我从本科大二开始就一直以Linux做主要开发环境，对深层次的编译、链接这些东西要比JVM虚拟机更加着迷（尽管知识水平并不允许我这么做）。因此，我也用业余时间将整个项目用C++重构了一次，尽管由于一些原因当时并没有完成，但我还是会在2020年的上半年再次迭代一个C++的正式版本，作为对自己实践能力的一次检验。
名著，主要就是阅读了C++ Primer、Effective C++和STL源码剖析这三本比较经典的，其中后两本读起来真是有一种酣畅淋漓的感觉，反倒是前一本可能是太过于大而全，有点长篇大论的啰嗦感。很感谢我的鹅厂师兄不厌其烦地帮我解惑，希望自己也早日达到这个高度。
结语 这次回家前也是去青岛中转了一下，找了找几位比较铁的同学了解下近况，畅谈了理想。有不堪工作繁重的，也有日子有奔头充满希望的，让还身为学生的我感触颇多。加上年后爆发的这次疫情以及种种意外，让人更加感叹世事的无常。活在当下，珍惜眼前，回去后好好撸码，勤去健身。</description>
    </item>
    
    <item>
      <title>2019 年中总结</title>
      <link>https://chongg039.cn/post/2019-mid-term-summary/</link>
      <pubDate>Sun, 12 May 2019 19:10:25 +0800</pubDate>
      
      <guid>https://chongg039.cn/post/2019-mid-term-summary/</guid>
      <description>今年上半年也快过完了，回顾一下半年来的一些工作，觉得也算是小有成长。
生物信息 说实话我不太想回忆起这部分经历，毕竟跨学科学习对于我一个毫无生信方面经验的计算机学生来说实在是太痛苦了，还是跨的生物。
最开始在和师兄一起做基因优化和密码子聚类的工作。说实话导师在密码子聚类方面的一些想法确实很不错，也曾让我激动过一阵。但经历过无数次实验验证后也渐渐熄灭了热情，剩下的只有接踵而来的失败和迷茫。师兄密码子优化方面的工作小有成果，并且能和川大国家重点实验室的同学们共事过一段时间，也是让我感到非常荣幸的一件事。
搁置了这块工作后，我的导师便让我转向了蛋白质结构预测的工作，并曾信心满满地说“我们一定要参加2020年的 CASP 大赛”，仿佛 Alpha-Fold 的成功明天就会发生在我们身上一般。没办法，赶鸭子上架也得学呀。在详细了解了这个比赛后，知道了大家使用的模型都是一个双层的残差网络，也顺利地在 Github 上找到了一个实现。模型训练出来后，老师比较高兴，但也没有什么后续的动作了。我也乐的清闲，能少跟生物沾边就少沾边。
上半年最后做的一个和生信相关的小项目是药物-靶点预测系统，这个的实现也是参考的清华的一篇 nature 的论文。从这个项目中第一次系统地接触到了推荐系统，原理部分结合上学期修的矩阵理论也没有什么理解上的难点。在实验室做的第一篇论文分享也是这个，总的来说收获较大，也许工作后会考虑去做推荐算法。不过项目本身按照我老师的脾性，仍然搁置了就是了。
上半年做的生信方面的工作也就这些，没有锻炼出生物专业方面的技能，杂的倒是学了不少。也很感谢我在海洋大学的同学和她的师兄们，给予了我很大的帮助。不过我还是想说一句，我对生信一丁点的兴趣都没得。
自然语言 很显然，和川大在生物方面项目进展的并不顺利，我就被发配去了自然语言组。我是十分高兴的，一方面是自己本身对 nlp 比较感兴趣，同时做 cv 人太多了，另一方面是原来生信方面的工作大部分也是在处理氨基酸序列和 DNA 序列，和自然语言语料有共通之处。
第一个做了一个实体关系抽取器，用在我们的项目中抽取实体关系三元组，但准确的来说实体抽取是师兄的工作，我这边只是做了一个关系分类器。大概的思路就是使用字向量+实体的位置向量编码，输入到一个双向的 LSTM 中训练，最后输出到 softmax 分类器中。因为是用的语料都是短句，所以最终模型表现也比较好。
在这个项目中我觉得收获还是很大的，因为从设计模型、处理语料、参数调整，到中间矩阵的维度确认，一行行敲出代码并做调试，前前后后耗费了一周多的时间。其中最复杂的地方我觉得是在数据预处理部分，模型也就是多个中间矩阵的维度计算比较麻烦（我是全部画出来并手动计算的，原谅我的菜）。说来惭愧，通过这个过程我才第一次才真正理解了网络中每一步在做什么。以前虽然调试过许多模型，看过许多相关的书和视频，你要是让我去说名每一层网络中的参数是多少，为什么是这样，我还真有可能答不上来。
师兄肯定了我的工作，然而当我拿去给导师看的时候，导师却认为统计模型和深度学习模型都不能达到我们项目中要求的接近 100% 的分类准确率，并认为依存句法分析就可以解决大部分问题，让我回去重新用句法分析器做。有了前几次的经历，这次我已经有了心理准备，但心里还是有点不爽。
那能怎么办呢，做呗，只能拿哈工大的 LTP 基于句法分析树尝试去抽取关系。但依存树都是从一个动词开始分析句子的依存结构，抽取出来的事实三元组并不是我们需要的（实体1，关系，实体2）这种。在尝试了许多方法后，发现依存句法只能解决一些非常简单的题目，对那种多个实体的复杂长句就非常难以设计分析路径了。抛开使用深度网络的方法，现在要想解决关系抽取问题，目前只能依靠句模和规则库来构建了。
总的来说，这一段时间在 nlp 方面也算入了门，最重要的是找到了一个比较喜欢的方向，有机会的话也希望将来能从事这方面相关的工作。
阅读 之前说过我买了一个 kindle ，也算汇报一下情况吧。
最近读完了吴军老师的两本书：数学之美和浪潮之巅。其实我主要想说一下这两本书的写作风格我非常喜欢，平铺直叙却又耐得住咀嚼。我本人是非常喜欢看纪录片的，而且也一直认为能把晦涩难懂的知识以一种简洁明快而不枯燥的方式向人们普及是一种非常了不起的能力，而吴军老师恰恰就拥有这种能力。
我其实不太喜欢写博文，觉得有这种时间不如用来学习一些新的知识。但是自从买了 kindle 后，静心阅读的时间变长了，利用手机等方式的碎片化阅读变少了，有了更多的时间去思考平时遇到的问题，整个人也变得不是那么的浮躁。
能把自己遇到的问题，自己的想法记录下来，既是温故，又能知新，还可以与他人碰撞思维的火花，何乐而不为？可能是受到吴军老师的影响，现在的我的确是这么想的。希望将来有一日，我也能以一种优美而简洁的语言写下自己的想法，分享给他人。</description>
    </item>
    
    <item>
      <title>2018 年终自检</title>
      <link>https://chongg039.cn/post/2018-summary/</link>
      <pubDate>Mon, 07 Jan 2019 14:14:55 +0800</pubDate>
      
      <guid>https://chongg039.cn/post/2018-summary/</guid>
      <description>这篇文章现在才写是因为复习矩阵考试而耽误了。在这里提一句感谢寒号鸟学长的知乎专栏以及实验室小伙伴的耐心指点，让我这个数学基础并不是那么好的学生勉强看懂了矩阵理论讲的内容。另外之前翻译的那本书鸽了一段时间，因为我实在没有想到成电的研一课业压力会这么大，也就没有选修随机过程。之后可能会继续进行翻译工作，不过可能进度会比较慢罢了。
2018年对我来说算是比较重要的一年。3月初回学校进行预面试，并且成功通过复式成为了（原）自动推理实验室的一员。整个过程还是很轻松愉快的，当时也感觉 f 老师是个很 nice 的人，虽然现在并不是这么觉得。四五月份抽出时间完成了水水的毕业设计，之后便做了一个比较私人的决定：去一趟稻城亚丁转换下心情，也算是做一次毕业旅行吧。
稻城白塔
牛奶海
牦牛兄弟
虽然恶劣的天体条件和贫瘠的山路让人身心匹配，不过稻城之行还是领略了一下藏区的风土人情，呼吸了一下高原干净的空气。回来后老师叫我去教研室我也找了个借口溜了，回家和爸钓了几天鱼。
研一的生活比我想象的要紧凑很多。我对 f 老师的评价是：一个理想主义的嘴炮王。具体为什么这么评价他就不谈了，估计当你的导师想要同时研究图推理、CV、nlp、小分子合成、基因优化、靶向药生成、量子机器学习的时候，你也会这么评价他。估计之后很长一段时间我也许就要去川大华西那边做实验去了。
相比本科时期的工作室生活，读研的第一个学期还是收获很多的，最起码机器学习方面的知识学习了不少。而且重拾了 C++ ，最近为了863也在重新写起了 Java 。虽然水平不高，但总算是在成长，视野也在开阔。以后也许在 z 师兄的指导下先搞两年 nlp ，毕竟也和 f 老师做的基因序列任务有点关系。前一段时间参加了 kaggle 的一个 CV 比赛，虽然没有实现一个很好的模型，但也算一次体验，之后有想法会继续参加。
读书方面，下半年快年底买了一个 kindle ，虽然目前只是用来看技术相关的书籍。作为我的第一台电子墨水设备体验很不错，以后争取多读一些文学作品。没有时间行万里路，那就争取做到读万卷书。
2019 年还得继续努力，人菜就要多学习。</description>
    </item>
    
  </channel>
</rss>
